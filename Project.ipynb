{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dat550 Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 12:58:02.200759: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-08 12:58:02.202326: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-08 12:58:02.209401: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-08 12:58:02.222394: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744109882.245989   94583 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744109882.251990   94583 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744109882.271031   94583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744109882.271077   94583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744109882.271079   94583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744109882.271081   94583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-08 12:58:02.278136: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textstat import textstat\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_threads = 8  # Adjust this based on the number of cores available\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_threads)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(num_threads)\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"preprocessing/data/articles-training-bypublisher.jsonl\"\n",
    "\n",
    "def load_json(filepath):\n",
    "\n",
    "    articles = []\n",
    "    table = str.maketrans(\"\",\"\",string.punctuation+\"“”‘’\")\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            title = data[\"title\"].lower().translate(table)\n",
    "            content = data[\"content\"].lower().translate(table)\n",
    "            articles.append({\n",
    "                \"id\": int(data[\"id\"]),\n",
    "                \"content\": f\"{title} {content}\"\n",
    "            })\n",
    "    \n",
    "\n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "\n",
    "\n",
    "def load_ground_truth(filepath):\n",
    "    return pd.read_json(filepath, orient=\"records\", lines=True)\n",
    "\n",
    "def merge_with_ground_truth(articles_df, ground_truth_df):\n",
    "    return articles_df.merge(ground_truth_df[['id', 'hyperpartisan']], on='id', how='left')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(article_path, truth_path):\n",
    "    print(\"Loading and merging data...\")\n",
    "    articles_df = load_json(article_path)\n",
    "    ground_truth_df = load_ground_truth(truth_path)\n",
    "    df = merge_with_ground_truth(articles_df, ground_truth_df)\n",
    "\n",
    "    # Filter out samples with missing labels\n",
    "    df = df.dropna(subset=['hyperpartisan'])\n",
    "    df['label'] = df['hyperpartisan'].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "\n",
    "    features = {\n",
    "        'sent_neg': sentiment['neg'],\n",
    "        'sent_pos': sentiment['pos'],\n",
    "        'sent_compound': sentiment['compound'],\n",
    "        'flesch': textstat.flesch_reading_ease(text),\n",
    "        'smog': textstat.smog_index(text),\n",
    "        'exclam': text.count('!'),\n",
    "        'questions': text.count('?'),\n",
    "        'quotes': text.count('\"'),\n",
    "        'length': len(text.split())\n",
    "    }\n",
    "\n",
    "    partisan_terms = {\n",
    "        'far_left': ['socialist', 'progressive', 'woke'],\n",
    "        'far_right': ['maga', 'conservative', 'patriot']\n",
    "    }\n",
    "\n",
    "    for group, terms in partisan_terms.items():\n",
    "        features[f'count_{group}'] = sum(text.count(term) for term in terms)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stylometric_features(df):\n",
    "    print(\"Extracting stylometric and sentiment features...\")\n",
    "    style_features = df['content'].progress_apply(extract_features)\n",
    "    return pd.DataFrame(style_features.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_vectorizer(texts, max_tokens=1000):\n",
    "    \"\"\"\n",
    "    Creates a text vectorizer using TensorFlow's TextVectorization layer\n",
    "    for a given set of texts. This will adapt the vectorizer based on the \n",
    "    text data to prepare for tokenization.\n",
    "    \n",
    "    Arguments:\n",
    "    - texts: List or Pandas Series of texts (articles) to process.\n",
    "    - max_tokens: Maximum number of tokens for the vectorizer (vocabulary size).\n",
    "    \n",
    "    Returns:\n",
    "    - vectorizer: The adapted TensorFlow TextVectorization layer.\n",
    "    \"\"\"\n",
    "    print(\"Creating TextVectorization layer...\")\n",
    "    vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode=\"tf_idf\",\n",
    "        ngrams=2\n",
    "    )\n",
    "    \n",
    "    # Batch processing to prevent memory overload\n",
    "    batch_size = 1000  # Adjust this based on your system's memory\n",
    "    text_batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n",
    "    \n",
    "    # Use tqdm to add a progress bar to the batch processing loop\n",
    "    for batch in tqdm(text_batches, desc=\"Adapting Vectorizer\", unit=\"batch\", total=len(text_batches)):\n",
    "        vectorizer.adapt(batch)\n",
    "    \n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "def vectorize_text_in_batches(texts, vectorizer, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Vectorizes the text in batches to manage memory usage efficiently.\n",
    "    \n",
    "    Arguments:\n",
    "    - texts: List or Pandas Series of texts to vectorize.\n",
    "    - vectorizer: The TensorFlow TextVectorization layer to use.\n",
    "    - batch_size: The size of each batch to process.\n",
    "    \n",
    "    Returns:\n",
    "    - Vectorized texts as a sparse tensor.\n",
    "    \"\"\"\n",
    "    text_batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n",
    "    all_vectors = []\n",
    "    \n",
    "    for batch in text_batches:\n",
    "        # Vectorize the current batch of texts\n",
    "        batch_vectorized = vectorizer(batch)\n",
    "        all_vectors.append(batch_vectorized)\n",
    "    \n",
    "    # Concatenate all the batches together into one large sparse tensor\n",
    "    all_vectors = tf.concat(all_vectors, axis=0)\n",
    "    \n",
    "    return all_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_and_combine(df, vectorizer):\n",
    "    X_text = vectorizer(df[\"content\"])\n",
    "    X_style = extract_stylometric_features(df)\n",
    "    X_style = X_style.astype(np.float32)\n",
    "    X_all = tf.concat([X_text, tf.convert_to_tensor(X_style.values)], axis=1)\n",
    "    return X_all.numpy(), X_style.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sklearn_model(model, X_train, X_test, y_train, y_test):\n",
    "    print(f\"\\nTraining {model.__class__.__name__}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\nClassification Report ({model.__class__.__name__}):\")\n",
    "    print(classification_report(y_test, preds))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_features(model, tfidf, X_style):\n",
    "    print(\"\\nTop Predictive Features:\")\n",
    "\n",
    "    # Get the vocabulary from the TextVectorization layer\n",
    "    feature_names = tfidf.get_vocabulary() + X_style\n",
    "    \n",
    "    # Choose the correct attribute based on model type\n",
    "    if hasattr(model, \"coef_\"):  # Linear models like Logistic Regression\n",
    "        importances = model.coef_[0]\n",
    "    elif hasattr(model, \"feature_importances_\"):  # Tree-based models like DT or RF\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        print(\"This model does not support feature importance inspection.\")\n",
    "        return\n",
    "\n",
    "    # Create and display feature importance dataframe\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', key=abs, ascending=False)\n",
    "\n",
    "    display(coef_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missclassified(model, X_test, y_test):\n",
    "    # Get misclassified samples\n",
    "    misclassified = X_test[y_test != model.predict(X_test)]\n",
    "    print(\"Number of misclassified samples:\", misclassified.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and merging data...\n"
     ]
    }
   ],
   "source": [
    "# Load and process articles\n",
    "df = prepare_data(\n",
    "    \"preprocessing/data/articles-training-bypublisher.jsonl\",\n",
    "    \"preprocessing/data/ground-truth-training-bypublisher.jsonl\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TextVectorization layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 13:25:29.280855: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "Adapting Vectorizer:   2%|▏         | 10/600 [00:41<41:12,  4.19s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vecotorizer_train = \u001b[43mcreate_text_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# X_train, X_style_cols_train = vectorize_and_combine(df,vecotorizer_train)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mcreate_text_vectorizer\u001b[39m\u001b[34m(texts, max_tokens)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Use tqdm to add a progress bar to the batch processing loop\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(text_batches, desc=\u001b[33m\"\u001b[39m\u001b[33mAdapting Vectorizer\u001b[39m\u001b[33m\"\u001b[39m, unit=\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m, total=\u001b[38;5;28mlen\u001b[39m(text_batches)):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vectorizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DAT550-Project/.venv/lib64/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:429\u001b[39m, in \u001b[36mTextVectorization.adapt\u001b[39m\u001b[34m(self, data, batch_size, steps)\u001b[39m\n\u001b[32m    427\u001b[39m         data = tf.expand_dims(data, -\u001b[32m1\u001b[39m)\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mself\u001b[39m.update_state(data)\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfinalize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DAT550-Project/.venv/lib64/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:435\u001b[39m, in \u001b[36mTextVectorization.finalize_state\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfinalize_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lookup_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DAT550-Project/.venv/lib64/python3.12/site-packages/keras/src/layers/preprocessing/index_lookup.py:687\u001b[39m, in \u001b[36mIndexLookup.finalize_state\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    682\u001b[39m     \u001b[38;5;28mself\u001b[39m.idf_weights_const = \u001b[38;5;28mself\u001b[39m.idf_weights.value()\n\u001b[32m    684\u001b[39m \u001b[38;5;66;03m# We call this here to save memory, now that we've built our vocabulary,\u001b[39;00m\n\u001b[32m    685\u001b[39m \u001b[38;5;66;03m# we don't want to keep every token we've seen in separate lookup\u001b[39;00m\n\u001b[32m    686\u001b[39m \u001b[38;5;66;03m# tables.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreset_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[38;5;28mself\u001b[39m._record_vocabulary_size()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DAT550-Project/.venv/lib64/python3.12/site-packages/keras/src/layers/preprocessing/index_lookup.py:697\u001b[39m, in \u001b[36mIndexLookup.reset_state\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    694\u001b[39m \u001b[38;5;28mself\u001b[39m.token_counts.remove(\u001b[38;5;28mself\u001b[39m.token_counts.export()[\u001b[32m0\u001b[39m])\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_mode == \u001b[33m\"\u001b[39m\u001b[33mtf_idf\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    696\u001b[39m     \u001b[38;5;28mself\u001b[39m.token_document_counts.remove(\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_document_counts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    698\u001b[39m     )\n\u001b[32m    699\u001b[39m     \u001b[38;5;28mself\u001b[39m.num_documents.assign(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DAT550-Project/.venv/lib64/python3.12/site-packages/tensorflow/python/ops/lookup_ops.py:2051\u001b[39m, in \u001b[36mMutableHashTable.export\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2048\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(name, \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m_lookup_table_export_values\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m   2049\u001b[39m                     [\u001b[38;5;28mself\u001b[39m.resource_handle]):\n\u001b[32m   2050\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m.resource_handle):\n\u001b[32m-> \u001b[39m\u001b[32m2051\u001b[39m     exported_keys, exported_values = \u001b[43mgen_lookup_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlookup_table_export_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2052\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresource_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_key_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m exported_keys, exported_values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DAT550-Project/.venv/lib64/python3.12/site-packages/tensorflow/python/ops/gen_lookup_ops.py:873\u001b[39m, in \u001b[36mlookup_table_export_v2\u001b[39m\u001b[34m(table_handle, Tkeys, Tvalues, name)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    872\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLookupTableExportV2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTkeys\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    876\u001b[39m     _result = _LookupTableExportV2Output._make(_result)\n\u001b[32m    877\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "vecotorizer_train = create_text_vectorizer(df[\"content\"])\n",
    "# X_train, X_style_cols_train = vectorize_and_combine(df,vecotorizer_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TextVectorization layer...\n",
      "Extracting stylometric and sentiment features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 628/628 [00:09<00:00, 64.02it/s]\n"
     ]
    }
   ],
   "source": [
    "test_articles_path = \"preprocessing/data/articles-test-byarticle.jsonl\"\n",
    "test_ground_truth_path = \"preprocessing/data/ground-truth-test-byarticle.jsonl\"\n",
    "\n",
    "# Load the test data and ground truth as DataFrames\n",
    "test_articles = pd.read_json(test_articles_path, orient=\"records\", lines=True)\n",
    "test_ground_truth = pd.read_json(test_ground_truth_path, orient=\"records\", lines=True)\n",
    "\n",
    "test_articles = test_articles.drop(columns=['hyperpartisan'])\n",
    "# Merge the two DataFrames\n",
    "test_data_df = pd.merge(test_articles, test_ground_truth[['id', 'hyperpartisan']], on='id')\n",
    "\n",
    "\n",
    "vecotorizer_test = create_text_vectorizer(test_data_df[\"content\"])\n",
    "X_test, X_style_cols_test = vectorize_and_combine(test_data_df,vecotorizer_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-split data\n",
    "# Or manually split just once and save using joblib.dump()\n",
    "y_train = df[\"label\"]\n",
    "y_test = test_data_df[\"hyperpartisan\"].astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Classification Report (LogisticRegression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.94      0.67       314\n",
      "           1       0.67      0.12      0.20       314\n",
      "\n",
      "    accuracy                           0.53       628\n",
      "   macro avg       0.59      0.53      0.43       628\n",
      "weighted avg       0.59      0.53      0.43       628\n",
      "\n",
      "\n",
      "Top Predictive Features:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "feature",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "importance",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "cac461d5-0595-44c8-aea6-b2cb8f0d08a9",
       "rows": [
        [
         "69",
         "—",
         "0.5300971678376944"
        ],
        [
         "958",
         "sponsored",
         "0.5233665544462829"
        ],
        [
         "296",
         "president trump",
         "0.4715876613628183"
        ],
        [
         "835",
         "12",
         "0.4091234418720594"
        ],
        [
         "407",
         "facebook",
         "-0.3831351593413076"
        ],
        [
         "37",
         "but",
         "0.3757385048706594"
        ],
        [
         "969",
         "bill clinton",
         "0.36853401373356515"
        ],
        [
         "382",
         "far",
         "0.3670013410588233"
        ],
        [
         "176",
         "may",
         "0.3655870467662639"
        ],
        [
         "162",
         "world",
         "-0.3654130637389706"
        ],
        [
         "61",
         "do",
         "-0.3636389454280168"
        ],
        [
         "33",
         "said",
         "-0.36294585747972274"
        ],
        [
         "221",
         "americans",
         "0.34723207013646645"
        ],
        [
         "389",
         "freedom",
         "-0.33995363009316043"
        ],
        [
         "3",
         "of",
         "0.3383302202678005"
        ],
        [
         "257",
         "with a",
         "0.3371504634654119"
        ],
        [
         "79",
         "now",
         "0.31864595662933415"
        ],
        [
         "467",
         "change",
         "0.3182242503140958"
        ],
        [
         "159",
         "2017",
         "-0.3156561013879268"
        ],
        [
         "493",
         "border",
         "-0.3155594492138697"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>—</td>\n",
       "      <td>0.530097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>sponsored</td>\n",
       "      <td>0.523367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>president trump</td>\n",
       "      <td>0.471588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>12</td>\n",
       "      <td>0.409123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>facebook</td>\n",
       "      <td>-0.383135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>but</td>\n",
       "      <td>0.375739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>bill clinton</td>\n",
       "      <td>0.368534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>far</td>\n",
       "      <td>0.367001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>may</td>\n",
       "      <td>0.365587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>world</td>\n",
       "      <td>-0.365413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>do</td>\n",
       "      <td>-0.363639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>said</td>\n",
       "      <td>-0.362946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>americans</td>\n",
       "      <td>0.347232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>freedom</td>\n",
       "      <td>-0.339954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>0.338330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>with a</td>\n",
       "      <td>0.337150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>now</td>\n",
       "      <td>0.318646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>change</td>\n",
       "      <td>0.318224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2017</td>\n",
       "      <td>-0.315656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>border</td>\n",
       "      <td>-0.315559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  importance\n",
       "69                 —    0.530097\n",
       "958        sponsored    0.523367\n",
       "296  president trump    0.471588\n",
       "835               12    0.409123\n",
       "407         facebook   -0.383135\n",
       "37               but    0.375739\n",
       "969     bill clinton    0.368534\n",
       "382              far    0.367001\n",
       "176              may    0.365587\n",
       "162            world   -0.365413\n",
       "61                do   -0.363639\n",
       "33              said   -0.362946\n",
       "221        americans    0.347232\n",
       "389          freedom   -0.339954\n",
       "3                 of    0.338330\n",
       "257           with a    0.337150\n",
       "79               now    0.318646\n",
       "467           change    0.318224\n",
       "159             2017   -0.315656\n",
       "493           border   -0.315559"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassified samples: 295\n"
     ]
    }
   ],
   "source": [
    "# Train & evaluate\n",
    "lr_model = LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        solver='liblinear'\n",
    "    )\n",
    "\n",
    "trained_lr = train_sklearn_model(lr_model,X_train,X_test,y_train,y_test)\n",
    "show_top_features(trained_lr, vecotorizer_train, X_style_cols_train)\n",
    "print_missclassified(trained_lr,X_test,y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training DecisionTreeClassifier...\n",
      "\n",
      "Classification Report (DecisionTreeClassifier):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67       314\n",
      "           1       0.67      0.53      0.59       314\n",
      "\n",
      "    accuracy                           0.63       628\n",
      "   macro avg       0.64      0.63      0.63       628\n",
      "weighted avg       0.64      0.63      0.63       628\n",
      "\n",
      "\n",
      "Top Predictive Features:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "feature",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "importance",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "532b9afc-1f8e-47a5-b6b2-996bf186df51",
       "rows": [
        [
         "3",
         "of",
         "0.18686876191846993"
        ],
        [
         "37",
         "but",
         "0.05472992330355988"
        ],
        [
         "1000",
         "sent_neg",
         "0.04701853274430077"
        ],
        [
         "1008",
         "length",
         "0.03738036104415373"
        ],
        [
         "958",
         "sponsored",
         "0.03539822251966962"
        ],
        [
         "233",
         "again",
         "0.030778112298969037"
        ],
        [
         "492",
         "cnn",
         "0.02437410332549544"
        ],
        [
         "107",
         "is a",
         "0.023970679944215676"
        ],
        [
         "319",
         "took",
         "0.021836195255797906"
        ],
        [
         "117",
         "before",
         "0.02075199892613938"
        ],
        [
         "33",
         "said",
         "0.020040695459939216"
        ],
        [
         "453",
         "the american",
         "0.019669221237539662"
        ],
        [
         "159",
         "2017",
         "0.019408066307055"
        ],
        [
         "510",
         "run",
         "0.01909252425698652"
        ],
        [
         "248",
         "is not",
         "0.01797802465476336"
        ],
        [
         "311",
         "the most",
         "0.017423856163600083"
        ],
        [
         "398",
         "making",
         "0.016424949226471742"
        ],
        [
         "776",
         "who have",
         "0.01587843328931287"
        ],
        [
         "25",
         "in the",
         "0.015392656565202101"
        ],
        [
         "988",
         "a few",
         "0.014829586096455016"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>0.186869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>but</td>\n",
       "      <td>0.054730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>sent_neg</td>\n",
       "      <td>0.047019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>length</td>\n",
       "      <td>0.037380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>sponsored</td>\n",
       "      <td>0.035398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>again</td>\n",
       "      <td>0.030778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>cnn</td>\n",
       "      <td>0.024374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>is a</td>\n",
       "      <td>0.023971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>took</td>\n",
       "      <td>0.021836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>before</td>\n",
       "      <td>0.020752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>said</td>\n",
       "      <td>0.020041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>the american</td>\n",
       "      <td>0.019669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2017</td>\n",
       "      <td>0.019408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>run</td>\n",
       "      <td>0.019093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>is not</td>\n",
       "      <td>0.017978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>the most</td>\n",
       "      <td>0.017424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>making</td>\n",
       "      <td>0.016425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>who have</td>\n",
       "      <td>0.015878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>in the</td>\n",
       "      <td>0.015393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>a few</td>\n",
       "      <td>0.014830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  importance\n",
       "3               of    0.186869\n",
       "37             but    0.054730\n",
       "1000      sent_neg    0.047019\n",
       "1008        length    0.037380\n",
       "958      sponsored    0.035398\n",
       "233          again    0.030778\n",
       "492            cnn    0.024374\n",
       "107           is a    0.023971\n",
       "319           took    0.021836\n",
       "117         before    0.020752\n",
       "33            said    0.020041\n",
       "453   the american    0.019669\n",
       "159           2017    0.019408\n",
       "510            run    0.019093\n",
       "248         is not    0.017978\n",
       "311       the most    0.017424\n",
       "398         making    0.016425\n",
       "776       who have    0.015878\n",
       "25          in the    0.015393\n",
       "988          a few    0.014830"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassified samples: 230\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "dt_model = DecisionTreeClassifier(class_weight='balanced', max_depth=12, random_state=42)\n",
    "trained_dt = train_sklearn_model(dt_model,X_train,X_test,y_train,y_test)\n",
    "show_top_features(trained_dt, vecotorizer_train, X_style_cols_train)\n",
    "print_missclassified(trained_dt,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RandomForestClassifier...\n",
      "\n",
      "Classification Report (RandomForestClassifier):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.85      0.75       314\n",
      "           1       0.80      0.57      0.67       314\n",
      "\n",
      "    accuracy                           0.71       628\n",
      "   macro avg       0.73      0.71      0.71       628\n",
      "weighted avg       0.73      0.71      0.71       628\n",
      "\n",
      "\n",
      "Top Predictive Features:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "feature",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "importance",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0ddf3637-baa3-4fef-9b05-3a7a3f1fd6e8",
       "rows": [
        [
         "1008",
         "length",
         "0.023579028106003243"
        ],
        [
         "3",
         "of",
         "0.019926452572557186"
        ],
        [
         "0",
         "[UNK]",
         "0.018430063932723565"
        ],
        [
         "8",
         "is",
         "0.017981127443564334"
        ],
        [
         "1003",
         "flesch",
         "0.01780169475072375"
        ],
        [
         "13",
         "it",
         "0.012388792749705202"
        ],
        [
         "4",
         "and",
         "0.011298584318083999"
        ],
        [
         "1001",
         "sent_pos",
         "0.011054673041150678"
        ],
        [
         "1",
         "the",
         "0.010717423779631214"
        ],
        [
         "5",
         "a",
         "0.010133312187044202"
        ],
        [
         "1000",
         "sent_neg",
         "0.010123499915583806"
        ],
        [
         "7",
         "that",
         "0.009460470821757355"
        ],
        [
         "37",
         "but",
         "0.008669847254530006"
        ],
        [
         "99",
         "these",
         "0.008384162744273121"
        ],
        [
         "18",
         "of the",
         "0.007473315546379917"
        ],
        [
         "42",
         "president",
         "0.007374120571549644"
        ],
        [
         "53",
         "if",
         "0.006520157776499729"
        ],
        [
         "2",
         "to",
         "0.005996509253725279"
        ],
        [
         "1010",
         "count_far_right",
         "0.005604338250793012"
        ],
        [
         "64",
         "so",
         "0.005488409968618214"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>length</td>\n",
       "      <td>0.023579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>0.019926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[UNK]</td>\n",
       "      <td>0.018430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>is</td>\n",
       "      <td>0.017981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>flesch</td>\n",
       "      <td>0.017802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>it</td>\n",
       "      <td>0.012389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>0.011299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>sent_pos</td>\n",
       "      <td>0.011055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>0.010717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>0.010133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>sent_neg</td>\n",
       "      <td>0.010123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>that</td>\n",
       "      <td>0.009460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>but</td>\n",
       "      <td>0.008670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>these</td>\n",
       "      <td>0.008384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>of the</td>\n",
       "      <td>0.007473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>president</td>\n",
       "      <td>0.007374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>if</td>\n",
       "      <td>0.006520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>0.005997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>count_far_right</td>\n",
       "      <td>0.005604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>so</td>\n",
       "      <td>0.005488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  importance\n",
       "1008           length    0.023579\n",
       "3                  of    0.019926\n",
       "0               [UNK]    0.018430\n",
       "8                  is    0.017981\n",
       "1003           flesch    0.017802\n",
       "13                 it    0.012389\n",
       "4                 and    0.011299\n",
       "1001         sent_pos    0.011055\n",
       "1                 the    0.010717\n",
       "5                   a    0.010133\n",
       "1000         sent_neg    0.010123\n",
       "7                that    0.009460\n",
       "37                but    0.008670\n",
       "99              these    0.008384\n",
       "18             of the    0.007473\n",
       "42          president    0.007374\n",
       "53                 if    0.006520\n",
       "2                  to    0.005997\n",
       "1010  count_far_right    0.005604\n",
       "64                 so    0.005488"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassified samples: 180\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', n_estimators=100, max_depth=12, random_state=42)\n",
    "trained_rf = train_sklearn_model(rf_model,X_train,X_test,y_train,y_test)\n",
    "\n",
    "show_top_features(trained_rf, vecotorizer_train, X_style_cols_train)\n",
    "print_missclassified(trained_rf,X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
