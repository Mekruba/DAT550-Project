{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dat550 Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_xml(\"Webis-data/extracted/ground-truth-training-byarticle-20181122.xml\")\n",
    "# df.to_json(\"preprocessing/data/ground-truth-training-byarticle.jsonl\",orient=\"records\",lines=True)\n",
    "\n",
    "# df = pd.read_xml(\"Webis-data/extracted/ground-truth-training-bypublisher-20181122.xml\")\n",
    "# df.to_json(\"preprocessing/data/ground-truth-training-bypublisher.jsonl\",orient=\"records\",lines=True)\n",
    "\n",
    "# df = pd.read_xml(\"Webis-data/extracted/ground-truth-test-bypublisher-20181212.xml\")\n",
    "# df.to_json(\"preprocessing/data/ground-truth-test-bypublisher.jsonl\",orient=\"records\",lines=True)\n",
    "\n",
    "# df = pd.read_xml(\"Webis-data/extracted/ground-truth-test-byarticle-20181207.xml\")\n",
    "# df.to_json(\"preprocessing/data/ground-truth-test-byarticle.jsonl\",orient=\"records\",lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"preprocessing/data/articles-training-byarticle.jsonl\"\n",
    "\n",
    "def load_json(filepath):\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            text = data[\"title\"] + \" \" + data[\"content\"] \n",
    "            articles.append({\n",
    "                \"id\": data[\"id\"],\n",
    "                \"text\": text\n",
    "            })\n",
    "    \n",
    "\n",
    "    return articles\n",
    "\n",
    "articles = load_json(filepath)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('trump', np.int64(2691)), ('said', np.int64(1393)), ('people', np.int64(1247)), ('clinton', np.int64(1139)), ('president', np.int64(1133)), ('just', np.int64(654)), ('like', np.int64(632)), ('hillary', np.int64(632)), ('police', np.int64(575)), ('white', np.int64(556)), ('new', np.int64(543)), ('obama', np.int64(540)), ('news', np.int64(537)), ('donald', np.int64(523)), ('donald trump', np.int64(501)), ('time', np.int64(495)), ('state', np.int64(485)), ('don', np.int64(465)), ('know', np.int64(452)), ('american', np.int64(433)), ('hillary clinton', np.int64(431)), ('america', np.int64(423)), ('right', np.int64(421)), ('twitter', np.int64(415)), ('did', np.int64(407)), ('left', np.int64(407)), ('man', np.int64(398)), ('media', np.int64(377)), ('house', np.int64(368)), ('year', np.int64(368)), ('country', np.int64(368)), ('fbi', np.int64(365)), ('think', np.int64(344)), ('going', np.int64(341)), ('say', np.int64(339)), ('women', np.int64(333)), ('years', np.int64(323)), ('world', np.int64(320)), ('com', np.int64(318)), ('make', np.int64(315)), ('election', np.int64(315)), ('2017', np.int64(308)), ('told', np.int64(301)), ('political', np.int64(301)), ('campaign', np.int64(294)), ('called', np.int64(293)), ('states', np.int64(291)), ('want', np.int64(290)), ('way', np.int64(286)), ('national', np.int64(280)), ('day', np.int64(267)), ('2016', np.int64(266)), ('free', np.int64(264)), ('according', np.int64(264)), ('government', np.int64(260)), ('law', np.int64(253)), ('black', np.int64(247)), ('video', np.int64(246)), ('north', np.int64(243)), ('story', np.int64(240)), ('says', np.int64(238)), ('article', np.int64(238)), ('united', np.int64(236)), ('men', np.int64(236)), ('old', np.int64(234)), ('public', np.int64(231)), ('support', np.int64(229)), ('investigation', np.int64(228)), ('americans', np.int64(227)), ('come', np.int64(220)), ('republican', np.int64(219)), ('white house', np.int64(217)), ('party', np.int64(215)), ('saying', np.int64(213)), ('mr', np.int64(212)), ('department', np.int64(211)), ('let', np.int64(211)), ('believe', np.int64(210)), ('doesn', np.int64(210)), ('president trump', np.int64(209)), ('times', np.int64(209)), ('does', np.int64(208)), ('democratic', np.int64(206)), ('long', np.int64(205)), ('group', np.int64(204)), ('good', np.int64(203)), ('woman', np.int64(203)), ('used', np.int64(202)), ('didn', np.int64(200)), ('money', np.int64(200)), ('use', np.int64(199)), ('today', np.int64(198)), ('press', np.int64(198)), ('democrats', np.int64(197)), ('things', np.int64(197)), ('ve', np.int64(197)), ('000', np.int64(194)), ('fact', np.int64(194)), ('family', np.int64(194)), ('united states', np.int64(193))]\n"
     ]
    }
   ],
   "source": [
    "def get_ngrams(text, n=2, top_k = 100):\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,n),stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "\n",
    "    freq = np.array(X.sum(axis=0)).flatten()\n",
    "    top_indices = freq.argsort()[-top_k:][::-1]\n",
    "    return [(features[i], freq[i]) for i in top_indices]\n",
    "\n",
    "texts = [article[\"text\"] for article in articles]\n",
    "ngrams = get_ngrams(texts)\n",
    "print(ngrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
